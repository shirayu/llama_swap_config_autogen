# llama-swap-config-autogen configuration file
# Edit this file to match your environment

# List of directories where models are stored (required)
models:
  - /opt/llama.cpp/models
  # - /home/user/models  # Add additional directories if any

# Basic settings
default_ttl: 300              # Model lifetime (seconds)
health_check_timeout: 240     # Health check timeout (seconds)
log_level: warn               # Log level (debug, info, warn, error)
start_port: 9091              # Starting port number

# Advanced settings (optional)
max_concurrent_models: 3      # Number of models that can run simultaneously
preload_popular_models: true  # Preload popular models
cleanup_interval: 60          # Cleanup interval (seconds)

# macros: Reusable command templates
macros:
  # Base settings
  "binary": "/opt/llama.cpp/bin/llama-server"
  "default-params": "--log-disable --jinja --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on ${layers-default} ${context-default}"
  "reasoning-base": "${default-params} --reasoning-format none"

  # Common parts: Context length
  "context-medium": "--ctx-size 32768"
  "context-large": "--ctx-size 65536"
  "context-ultra": "--ctx-size 131072"
  "context-full": "-c 0"
  "context-default": "${context-medium}"

  # Common parts: GPU settings
  "layers-mini": "--n-gpu-layers 13"
  "layers-medium": "--n-gpu-layers 25"
  "layers-full": "--n-gpu-layers 999"
  "layers-default": "${layers-full}"

  # Common parts: CPU settings
  "cpu-offload-ffn-full": "-ot '.*.ffn_.*_exps.=CPU'"
  "cpu-offload-moe-max": "--cpu-moe"
  "cpu-offload-moe-12": "--n-cpu-moe 12"
  "cpu-offload-moe-24": "--n-cpu-moe 24"
  "cpu-offload-moe-36": "--n-cpu-moe 36"
  "cpu-offload-moe-default": "${cpu-offload-moe-12}"

  # Common parts: Temperature settings
  "temp-reasoning": "--temperature 0.7 --top-p 0.9"
  "temp-conding": "--temperature 0.1"

  # Common parts: Memory settings
  "low-memory": "--no-mmap --mlock"
  "memory-efficient": "--cache-type-k q4_0 --cache-type-v q4_0"

  # Common parts: Performance settings
  "inference-fast": "--batch-size 512 --ubatch-size 512"

  # Model-specific macros
  "gpt-oss-20b-params": "${reasoning-base} -c 0 -fa"
  "deepseek-r1-params": "${reasoning-base} ${temp-reasoning}"
  "gemma-large-params": "${default-params} ${layers-medium} ${context-medium}"
  "gpt-oss-120b-params": "${reasoning-base} ${layers-mini} --ctx-size 16384 --no-mmap"
  "qwen-coder-params": "${default-params} ${temp-coding} ${context-medium}"
  "qwen-cpu-params": "${default-params} ${cpu-offload-moe-default}"
  "qwen-context-large-params": "${default-params} ${context-large}"

  # New model macros
  "memory-constrained-params": "${default-params} ${low-memory} ${memory-efficient}"

  # Combined macros
  "long-context-params": "${default-params} ${context-large}"
  "cpu-long-context-params": "${default-params} ${cpu-offload-moe-default} ${context-full}"

# Model name patterns and macro mapping
model_patterns:
  # Reasoning-focused models
  "DeepSeek-R1-Distill": "deepseek-r1-params"
  "gpt-oss-120b": "gpt-oss-120b-params"
  "gpt-oss-20b": "gpt-oss-20b-params"

  # General-purpose large models
  "Qwen3-30B-A3B-Instruct-2507": "qwen-context-large-params"
  "gemma-3-27b": "gemma-large-params"

  # Coding-focused models
  "Qwen3-Coder": "qwen-coder-params"
  "StarCoder": "qwen-coder-params"

  # Lightweight models (for memory-constrained environments)
  "Phi-3": "memory-constrained-params"
  "TinyLlama": "memory-constrained-params"

# Variant generation settings (optional)
# Automatically generate variations of the same model with different settings
variants:
  # CPU variant
  - base_pattern: "Qwen3-30B-A3B-Instruct-2507"
    suffix: " (with CPU)"
    macro: "qwen-cpu-params"

  # Combined variant
  - base_pattern: "Qwen3-30B-A3B-Instruct-2507"
    suffix: " (CPU + Long Context)"
    macro: "cpu-long-context-params"

  # Memory-constrained variant
  - base_pattern: "gemma-3-27b"
    suffix: " (Memory Efficient)"
    macro: "memory-constrained-params"

  # Fast inference variant
  - base_pattern: "DeepSeek-R1-Distill"
    suffix: " (Fast)"
    macro: "${deepseek-r1-params} ${inference-fast}"

