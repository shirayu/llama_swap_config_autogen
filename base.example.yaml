models:
- /opt/data/llm/models
default_ttl: 300
health_check_timeout: 240
log_level: warn
start_port: 9091
max_concurrent_models: 3
preload_popular_models: true
cleanup_interval: 60
macros:
  binary: /app/llama-server
  default-params:  --jinja --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on ${layers-default} ${context-default}
  reasoning-base: ${default-params} --reasoning-format none
  context-short: --ctx-size 4096
  context-medium: --ctx-size 32768
  context-large: --ctx-size 65536
  context-ultra: --ctx-size 131072
  context-full: -c 0
  context-default: ${context-medium}
  layers-mini: --n-gpu-layers 13
  layers-medium: --n-gpu-layers 25
  layers-full: --n-gpu-layers 999
  layers-default: ${layers-full}
  cpu-offload-ffn-full: -ot '.*.ffn_.*_exps.=CPU'
  cpu-offload-moe-max: --cpu-moe
  cpu-offload-moe-6: --n-cpu-moe 6
  cpu-offload-moe-12: --n-cpu-moe 12
  cpu-offload-moe-24: --n-cpu-moe 24
  cpu-offload-moe-36: --n-cpu-moe 36
  cpu-offload-moe-default: ${cpu-offload-moe-12}
  cpu-offload-threads: --threads 16
  temp-reasoning: --temp 0.7 --top-p 0.9
  temp-conding: --temp 0.1
  low-memory: --no-mmap --mlock
  memory-efficient: --cache-type-k q4_0 --cache-type-v q4_0
  inference-fast: --batch-size 512 --ubatch-size 512
  gpt-oss-20b-params: ${reasoning-base} -c 0 -fa
  deepseek-r1-params: ${reasoning-base} ${temp-reasoning}
  gemma-large-params: ${default-params} ${layers-medium} ${context-medium}
  gpt-oss-120b-params: ${reasoning-base} ${layers-mini} --ctx-size 16384 --no-mmap
  qwen-coder-params: ${default-params} ${temp-coding} ${context-medium}
  qwen-cpu-params: ${default-params} ${cpu-offload-moe-default} ${cpu-offload-threads} ${context-large}
  short-ctx-params: ${default-params} ${context-short}
  qwen-cpu-params-big: ${default-params} --n-gpu-layers 15 ${cpu-offload-moe-default} ${cpu-offload-threads} ${context-large}
  qwen-context-large-params: ${default-params} ${context-large}
  memory-constrained-params: ${default-params} ${low-memory} ${memory-efficient}
  long-context-params: ${default-params} ${context-large}
  glm-47-params: ${default-params} --repeat-penalty 1.0 --temp 1.0
  glm-47-thinking-off-params: >-
    ${glm-47-params} --chat-template-kwargs '{"enable_thinking": false}'
model_patterns:
  DeepSeek-R1-Distill: deepseek-r1-params
  gpt-oss-120b: gpt-oss-120b-params
  gpt-oss-20b: gpt-oss-20b-params
  glm-47-flash: glm-47-params
  Qwen3-30B-A3B-Instruct-2507: qwen-context-large-params
  Qwen3-Next-80B-A3B: qwen-context-large-params
  gemma-3-27b: gemma-large-params
  Qwen3-Coder: qwen-coder-params
  StarCoder: qwen-coder-params
  Phi-3: memory-constrained-params
  TinyLlama: memory-constrained-params
variants:
- base_pattern: Qwen3-30B-A3B-Instruct-2507
  suffix: ' (with CPU)'
  macro: qwen-cpu-params
- base_pattern: Qwen3-30B-A3B-Instruct-2507
  suffix: ' (short ctx)'
  macro: short-ctx-params
- base_pattern: gemma-3-27b-it-qat
  suffix: ' (short ctx)'
  macro: short-ctx-params
- base_pattern: DeepSeek-R1-Distill-Qwen-32B
  suffix: ' (short ctx)'
  macro: short-ctx-params
- base_pattern: tokyotech-llm-Llama-3.3-Swallow-70B
  suffix: ' (short ctx)'
  macro: short-ctx-params
- base_pattern: Qwen3-Next-80B-A3B
  suffix: ' (with CPU)'
  macro: qwen-cpu-params-big
- base_pattern: glm-4.7-flash
  suffix: ' (thinking off)'
  macro: glm-47-thinking-off-params
- base_pattern: gemma-3-27b
  suffix: ' (Memory Efficient)'
  macro: memory-constrained-params
- base_pattern: DeepSeek-R1-Distill
  suffix: ' (Fast)'
  macro: ${deepseek-r1-params} ${inference-fast}
